{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import threading\n",
    "import webcolors\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "import face_recognition\n",
    "\n",
    "from tqdm import tqdm\n",
    "from imutils import paths\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from darkflow.net.build import TFNet\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from webcolors import rgb_percent_to_hex, hex_to_name, hex_to_rgb, rgb_to_hex, css3_hex_to_names\n",
    "\n",
    "DATASET_FILE_NAME = 'model_encodings.pickle'\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "pool = ThreadPool(processes=10)\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = image.reshape((image.shape[1] * image.shape[0], 3))\n",
    "\n",
    "    clt = KMeans(n_clusters = 8)\n",
    "    clt.fit(image)\n",
    "    hist = centroid_histogram(clt)\n",
    "    cluster_centers = list(clt.cluster_centers_)\n",
    "    labs = ['#' + ''.join([\"%0.2X\"%(j) for j in i.astype(\"uint8\").tolist()]) for i in clt.cluster_centers_]\n",
    "\n",
    "    INDEX = list(hist).index(max(hist))\n",
    "\n",
    "    return (labs[INDEX],  clt.cluster_centers_[INDEX][::-1])\n",
    "\n",
    "def _hex_to_name(requested_colour):\n",
    "    try:\n",
    "        closest_name = actual_name = hex_to_name(requested_colour)\n",
    "    except ValueError:\n",
    "        closest_name = closest_colour(requested_colour)\n",
    "        actual_name = None\n",
    "    return actual_name, closest_name\n",
    "\n",
    "def closest_colour(requested_colour):\n",
    "    min_colours = {}\n",
    "    try:\n",
    "        requested_colour = hex_to_rgb(requested_colour)\n",
    "    except:\n",
    "        pass\n",
    "    for key, name in css3_hex_to_names.items():\n",
    "        r_c, g_c, b_c = hex_to_rgb(key)\n",
    "        rd = (r_c - requested_colour[0]) ** 2\n",
    "        gd = (g_c - requested_colour[1]) ** 2\n",
    "        bd = (b_c - requested_colour[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_bg(img):\n",
    "  hMin = 29  # Hue minimum\n",
    "  sMin = 30  # Saturation minimum\n",
    "  vMin = 0   # Value minimum (Also referred to as brightness)\n",
    "  hMax = 179 # Hue maximum\n",
    "  sMax = 255 # Saturation maximum\n",
    "  vMax = 255 # Value maximum\n",
    "  # Set the minimum and max HSV values to display in the output image using numpys' array function. We need the numpy array since OpenCVs' inRange function will use those.\n",
    "  lower = np.array([hMin, sMin, vMin])\n",
    "  upper = np.array([hMax, sMax, vMax])\n",
    "  # Create HSV Image and threshold it into the proper range.\n",
    "  hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # Converting color space from BGR to HSV\n",
    "  mask = cv2.inRange(hsv, lower, upper) # Create a mask based on the lower and upper range, using the new HSV image\n",
    "  # Create the output image, using the mask created above. This will perform the removal of all unneeded colors, but will keep a black background.\n",
    "  output = cv2.bitwise_and(img, img, mask=mask)\n",
    "  # Add an alpha channel, and update the output image variable\n",
    "  *_, alpha = cv2.split(output)\n",
    "  dst = cv2.merge((output, alpha))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say(label):\n",
    "    try:\n",
    "        engine.say(label)\n",
    "        engine.runAndWait()\n",
    "        engine.stop()\n",
    "    except: # Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_with_colors = ['car', 'motorbike', \"chair\", \"bird\", \"cat\", \"cat\", \"dog\", \"horse\", \"cow\", \"umbrella\",\"handbag\", \"tie\",\"suitcase\",\"frisbee\",\"kite\",\"skateboard\",\"bottle\",\"cup\",\"fork\",\"knife\",\"spoon\"\"cell phone\",\"knife\",\"sofa\",\"mouse\",\"cake\",\"clock\",\"toothbrush\",'mouse','bottle','laptop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ready!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [tuple(255 * np.random.rand(3)) for i in range(7)]\n",
    "\n",
    "def predict(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return tfnet.return_predict(img)\n",
    "\n",
    "def fetchMetaInformation(result, image):\n",
    "    label = result['label']\n",
    "\n",
    "    tl = (result['topleft']['x'],result['topleft']['y'])\n",
    "    br = (result['bottomright']['x'],result['bottomright']['y'])\n",
    "    center = ((tl[1]+br[1]) // 2 , (tl[0]+br[0]) // 2)\n",
    "    shape = image.shape\n",
    "    \n",
    "    h1_3 = shape[0] // 3\n",
    "    w1_3 = shape[1] // 3\n",
    "\n",
    "    pos = [\n",
    "        ((0, 0), \"top left\"),\n",
    "        ((0, w1_3), \"top center\"),\n",
    "        ((0, 2 * w1_3), \"top right\"),\n",
    "        \n",
    "        ((h1_3, 0), \"middle left\"),\n",
    "        ((h1_3, w1_3), \"middle center\"),\n",
    "        ((h1_3, 2 * w1_3), \"middle right\"),\n",
    "\n",
    "        ((2 * h1_3, 0), \"bottom left\"),\n",
    "        ((2 * h1_3, w1_3), \"bottom center\"),\n",
    "        ((2 * h1_3, 2 * w1_3), \"bottom right\")\n",
    "    ]\n",
    "\n",
    "    for i in pos:\n",
    "        p = i[0]\n",
    "\n",
    "        if (\n",
    "            (p[0] < center[0] and p[1] < center[1]) and\n",
    "            ((center[0] < (p[0] + h1_3)) and (center[1] < (p[1] + w1_3)))\n",
    "        ):\n",
    "            label += \" in %s\" % (i[1])\n",
    "            break\n",
    "        \n",
    "    if label in object_with_colors:\n",
    "        try:\n",
    "\n",
    "            color = cv2.resize(clipped, (1,1))[0][0]\n",
    "\n",
    "#             clipped = rm_bg(clipped)\n",
    "            actual =_hex_to_name(rgb_to_hex(color))[1]\n",
    "            return \"%s %s\" % (actual, label)\n",
    "        except:\n",
    "            actual = ''\n",
    "            pass\n",
    "\n",
    "    return label\n",
    "\n",
    "def recz(img, prev_output):\n",
    "    label=\"\"\n",
    "\n",
    "    stime = time.time()\n",
    "    results = predict(img)\n",
    "\n",
    "    out_string = \"There is \"\n",
    "    c = 0\n",
    "\n",
    "    for clr, result in zip(colors, results):\n",
    "        tl = (result['topleft']['x'], result['topleft']['y'])\n",
    "        br = (result['bottomright']['x'], result['bottomright']['y'])\n",
    "        label = fetchMetaInformation(result, img)\n",
    "\n",
    "        out_string += \"a %s, \" % (label)\n",
    "        \n",
    "        img = cv2.rectangle(img, tl, br, clr, 7)\n",
    "        img = cv2.putText(img, label, tl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    out_string = out_string[:-2]\n",
    "\n",
    "    if(c > 0 and prev_output != out_string):\n",
    "        threading.Thread(target=say, args=(out_string + \".\", )).start()\n",
    "        prev_output = out_string\n",
    "\n",
    "    fps = 1 / (time.time() - stime)\n",
    "\n",
    "    img = cv2.putText(\n",
    "        img,\n",
    "        'FPS {:.1f}'.format(fps),\n",
    "        (10, 40),\n",
    "        cv2.FONT_HERSHEY_COMPLEX,\n",
    "        .8,\n",
    "        (255, 255, 255),\n",
    "        2\n",
    "    )\n",
    "    \n",
    "    print('\\rFPS %.1f: %s.%s' % (fps, out_string, \" \" * 30), end=\"\\r\")\n",
    "\n",
    "    return (img, prev_output)\n",
    "\n",
    "\"Ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/yolo.cfg\n",
      "Parsing cfg/yolo.cfg\n",
      "Loading bin/yolo.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 0.023951053619384766s\n",
      "Model has a coco model name, loading coco labels.\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 608, 608, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "GPU mode with 0.22 usage\n",
      "Finished in 8.455344438552856s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = .5\n",
    "options = {\n",
    "    'model': 'cfg/yolo.cfg',\n",
    "    'load': 'bin/yolo.weights',\n",
    "    'threshold': accuracy,\n",
    "    'gpu': 0.22\n",
    "}\n",
    "tfnet = TFNet(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture from camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS 12.1: There is a person in middle right, a person in middle center.                                                             \r"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "pool = ThreadPool(processes=10)\n",
    "prev_output = \"\"\n",
    "\n",
    "engine.say(\"Started\")\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    img, prev_output  = recz(frame, prev_output)\n",
    "\n",
    "    cv2.imshow('ImageWindow', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture from window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS 7.5: There i.                                                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "import win32gui\n",
    "from PIL import ImageGrab\n",
    "\n",
    "# hwnd = win32gui.FindWindow(None, r'192.168.43.56:4747/video')\n",
    "# hwnd = win32gui.FindWindow(None, r'DroidCam Video Feed')\n",
    "# hwnd = win32gui.FindWindow(None, r'video - VLC Media Player')\n",
    "hwnd = win32gui.FindWindow(None, r'DroidCam Video')\n",
    "dimensions = win32gui.GetWindowRect(hwnd)\n",
    "# win32gui.SetForegroundWindow(hwnd)\n",
    "\n",
    "pool = ThreadPool(processes=3)\n",
    "prev_output = \"\"\n",
    "\n",
    "engine.say(\"Started\")\n",
    "\n",
    "while(True):\n",
    "    image = ImageGrab.grab(dimensions)\n",
    "    img = np.array(image)\n",
    "    img = img[:, :, ::-1].copy()\n",
    "    \n",
    "    img = img[110: img.shape[0]-10, 10:img.shape[1]-10]\n",
    "\n",
    "    img, prev_output  = recz(img, prev_output)\n",
    "\n",
    "    cv2.imshow('ImageWindow', img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit16b3547e794943fdaee56569cafc8197"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
